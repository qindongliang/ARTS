
 [Algorithm 922. Sort Array By Parity II](#algorithm)

 [Review 深入理解Trie树](#review)

 [Technique Spring Boot如何集成Nginx配置代理 ](#technique)

 [Share 深入理解什么是LSM-Tree](#share)


# Algorithm
 https://leetcode.com/problems/find-common-characters/
 
 ```java
package leetcode.easy.array_all;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

/***
 * https://leetcode.com/problems/find-common-characters/
 * 给定一个全英文字母小写的字符串数组
 * 统计出里面最小的公共字符的个数，最后输出到一个List<String>里面
 * 举例
 * [aooo, aoo, boo] , 输出是oo，注意oo需要拆分成2个字符放入List<String>里面
 */
public class FindCommonsCharacters {


    public static List<String> commonChars(String[] A) {

        List<String> ans=new ArrayList<>();
        //声明数组，用来保存最终的公共字符的个数
        int count[]=new int[26];
        //初始化填充最大值，便于统一处理
        Arrays.fill(count,Integer.MAX_VALUE);

        for (String str:A){

            // 对每一个字符串声明一个字符数组
            int[] cnt=new int[26];
            //统计在这一个字符串里面，每个字符出现的字数
            for(char c:str.toCharArray()){
                cnt[c-'a']++;
            }

            //拿当前字符串的数组统计值，与count数组里面的统计值进行比较，取最小值就是公共字符（木桶短板决定）
            //注意，如果第一个字符串里面出现某个字符，第二个字符串里面没有出现，那么取最小值就是没有出现，这一点需要注意
            for (int i = 0; i < 26; i++) {
                count[i]=Math.min(count[i],cnt[i]);
            }

        }

        //依次循环处理完所有的字符串后，进行遍历输出组装，就是所有的公共字符
        for(char c='a';c<='z';c++){
            while (count[c-'a']-->0){
                ans.add(c+"");
            }

        }

        return ans;
    }


    public static void main(String[] args) {

        System.out.println(commonChars(new String[]{"bella","label","roller"}));

    }



}

```

# Review

https://leetcode.com/problems/implement-trie-prefix-tree/

https://www.ideserve.co.in/learn/trie-delete

### 前言
前面的文章介绍过各种高效的的数据结构，比如二叉搜索树，AVL树，红黑树，B树，跳跃表等，今天我们再来学习一种多路树，叫做Trie树。


### 什么是Trie树

在计算机科学中，Trie，又称前缀树或字典树，是一种有序树，用于保存关联数组。其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。Trie树的名称来源于搜索引擎中的专有名词的retrieval，发音和单词try一样。

比如下面的这个Trie树包含“Cat”，“Cut”，“Cute”，“To”，“B”五个单词，其存储图示如下：

[img1]

从上面我们可以发现，前缀一样的单词，在实际存储中只存储了一份，并且如果单词后面有.符号表从root到这个位置是一个单词，前缀相同的单词会复用一样的公共节点。


###  Trie树的应用场景

Trie最典型的应用场景是用于搜索引擎的suggest功能，比如我们在google中，每输入一个英文字母，搜索引擎都会给过我们返回以这个字母为前缀的相关的结果，如下：

[img02]

除以之外，还有常见的IDE里面自动补全功能和各种通讯录的自动补全功能等。

### Trie树的工作原理

这里以英文单词为例，我们知道英语单词由26个字母组成，每一个字母都是这26个字母中的其中一个，假如现在我们想为英语单词的suggest功能，那么使用Trie树就非常适合。因为总共的可能性只有26，所以我们可以定义一个长度为26的数组来存储所有的可能性，然后附带一个boolean类型的变量，来标记当前层是否是一个单词。

如下定义：

```
   static   class TrieNode{
       TrieNode[] children;
       boolean isWord;

       public TrieNode(){
           children=new TrieNode[26];
       }
    }
```

#### 如何插入

Trie树的结构，通常需要一个虚拟的head节点来辅助，head节点并不存储数据，仅仅用于操作方便，在插入的时候，会分解单词为一个字符数组，然后依次插入其中的每一个字母到Trie树里面，如果插入的位置不存在该字母，那么代表第一次插入，就需要新建一个TrieNode节点， 如果插入的地方已经存在，那么就直接继续插入下一个字母，直到整个单词的每一个字母都插入完毕后，在最后一个TrieNode节点处标记到目前这个节点处，代表一个完整的单词。


#### 如何查询

查询主要有两种形式，第一种是判断是否存在某个单词在Trie树里面，第二种是判断指定的前缀是否在Trie树里面存在。

这两种case的检索方式大致一样，就是从head节点入手，判断这个单词的第一个字母是否存在，如果就跳到第二级继续搜索，知道遍历完整个字母，返回最后一个节点，然后判断如果该节点有数据，并且有完整单词标记，那么就证明该单词存在，如果没有单词标记，那么证明该前缀存在。如果判断返回的节点没有数据，那么就证明当前的Trie树里面不包含某个单词或者输入的指定的前缀。


#### 如何删除

Trie树的相对来说稍微复杂点，举个例子，比如包含6个单词的Trie树：
```
("abc")
("xy")
("xyz")
("abb")
("xyzb"), 
("word")
```
图示如下：

[img03]

我们看删除的几种情况：

（1）如果要删除的单词不存在，则不做任何操作

（2）如果要删除的单词是没有任何字母被作为公共前缀，那么就要删除每个字母，如上图的单词word

（3）如果要删除的单词全部字母都是公共前缀，那么仅仅在这个单词的尾部标记不是完整单词即可，如上图的单词xyz

（4）如果要删除的单词是超出了公共前缀，那么仅仅删除多出的部分即可，如上图的xyzb，在删除的时候仅仅删除字母b即可。

（5）如果要删除的单词是两条路径的公共前缀，那么仅仅删除非公共前缀的部分即可，这种情况与（4）类似，但不同的是（4）是在一条路径上，而（5）是在两条路径上，如上图要删除abc，因为前缀ab是共用的，所以仅仅删除abc的c字母即可。


上面就是删除的全部情况，不过在Trie树里面，重要的部分是插入和检索部分，删除部分可能比较少的使用。




### Trie树的实现方式

Trie树有两种实现策略，第一种使用数组存储所有的可能性，第二种是使用的Map存储所有的可能性，使用数组的方式需要对存储的字符和数组的index之间要有明确的映射规则，这样便于查询，比如如果想做中文的suggest，一种的可行的办法就是将整个中文字符集映射到具体的数值上，然后与上面字母的操作类似，如果选择使用Map作为可能性存储，相对来说会更加灵活一点，毕竟Map在大多数编程语言里面都有封装好的动态实现。

下面给出的是基于数组实现Trie树的方式，基于Map的实现方式可以参考我的github的代码：

https://github.com/qindongliang/Java-Note

 
```

public class TrieByArray {


   static   class TrieNode{
       TrieNode[] children;//存储所有的可能性
       boolean isWord;//是否是一个完整单词
       public TrieNode(){
           children=new TrieNode[26];
       }
    }


    TrieNode root;

   public TrieByArray(){
       root=new TrieNode();
   }


   public void insert(String word){

       TrieNode prev=root;

       for (int i = 0; i < word.length(); i++) {

           if(prev.children[word.charAt(i)-'a']==null){
               prev.children[word.charAt(i)-'a']=new TrieNode();
           }

           prev=prev.children[word.charAt(i)-'a'];

       }

       prev.isWord =true;

   }

   public boolean search(String word){
       TrieNode result=checkExists(word);
       if(result!=null&&result.isWord){
           return true;
       }
       return false;

   }


   public TrieNode checkExists(String query){

       TrieNode cur=root;

       for (int i = 0; i < query.length(); i++) {
           //待查询的字符不存在
           if(cur.children[query.charAt(i)-'a']==null){
               return cur.children[query.charAt(i)-'a'];
           }else{
               cur=cur.children[query.charAt(i)-'a'];;
           }
       }

       return cur;
   }


   public void delete(String key){
     if(root==null||key==null){
         return;
     }

       deleteWord(root,key,key.length(),0);

   }

   private boolean hasChildren(TrieNode currentNode){
       for (int i = 0; i < currentNode.children.length; i++) {
           if(currentNode.children[i]!=null){
               return true;
           }
       }
       return false;
   }


   public boolean deleteWord(TrieNode currentNode,String word,int length,int level){
       boolean deletedSelf=false;
       if(level==length){
            //如果当前节点是最后，并且没有孩子节点
           if(!hasChildren(currentNode)){
               currentNode=null;
               deletedSelf=true;
           }else{//有孩子节点
               currentNode.isWord =false;//则将其置为非单词属性即可
               deletedSelf=false;
           }

       }else {
           //由下而上递归删除
           TrieNode childNode=currentNode.children[word.charAt(level)-'a'];
           boolean childDeleted=deleteWord(childNode,word,length,level+1);
           if(childDeleted){//如果单词的最后的字符没有孩子节点，就可以被删除，然后需要继续向上递归判断其前一个字符是否是需要删除
               currentNode.children[word.charAt(level)-'a']=null;//设置子节点为null
               if(currentNode.isWord){//判断父节点是否是一个word的结束，如果是说明是公共前缀就不能再删除了
                   deletedSelf=false;
               }else if(hasChildren(currentNode)){//如果这个父节点还有孩子节点，说明也是公共前缀，也不能再删除了
                  deletedSelf=false;
               }else{//到这一步，说明父节点也是要删除单词唯一的的字符，可以继续向上删除
                   currentNode=null;
                   deletedSelf=true;
               }
           }else {//如果不需要被删除，则向上传递false即可
               deletedSelf=false;
           }
       }

       return deletedSelf;
   }



   public boolean startsWith(String prefix){
       TrieNode result=checkExists(prefix);
       if(result!=null){
           return true;
       }
       return false;
    }







    public static void main(String[] args) {

       TrieByArray trie=new TrieByArray();
       trie.example1();
//       trie.example2();
//       trie.example3();


   }

   private void example1(){
       TrieByArray trie = new TrieByArray();

        trie.insert("apple");
        System.out.println(trie.search("apple"));   // returns true

        System.out.println(trie.search("app"));     // returns false
        System.out.println(trie.startsWith("app")); // returns true
        trie.insert("app");
        System.out.println(trie.search("app"));     // returns true
   }

   private void example2(){
       TrieByArray trie = new TrieByArray();

       trie.insert("gat");
       trie.insert("gag");
       System.out.println(trie.search("gat"));//true
       System.out.println(trie.startsWith("ga"));//true
       trie.delete("gat");
       System.out.println(trie.search("gat"));//false
       System.out.println(trie.startsWith("ga"));//true
       System.out.println(trie.search("gag"));//true
   }

   private void example3(){
       TrieByArray trie = new TrieByArray();
       trie.insert("abcd");
       trie.insert("abc");

       System.out.println(trie.search("abc"));
       System.out.println(trie.startsWith("abc"));

       trie.delete("abc");

       System.out.println(trie.search("abc"));
       System.out.println(trie.search("abcd"));
       System.out.println(trie.startsWith("abc"));
   }


}

```

### Trie树的时间和空间复杂度分析

Trie树的时间复杂度为O（k），k=该字符串（单词或者前缀）的长度，在最好最坏的case均为O（k）。

Trie树的空间复杂度，如果重复的前缀比较多，那么一定程度上使用Trie是可以节省存储空间的，但如果重复的前缀不多，这样就会导致Trie消耗大量内存，为什么这么说？

还记得上面数组的实现方式吗？在TrieNode里面需要把每一种可能性都要提前存储到一个数组，方便查找，拿英文单词为例，一个单词cat，看起来只需要3个char字符的空间就可以了，但实际上每个字符都需要存储一个26大小的指针数组，这样就需要O（n*k）的空间来存储，k=字符串的长度，n=共有多少个这样不重复的字符，这么算下来，其实Trie树的原理，可以看做是拿空间换时间的实现策略。

那么有的同学会说使用数组的方式为了查找快速，没办法只能那么存，那么使用Map的方式是不是就节省内存了？其实并不是，不要忘记，Map的底层也是通过数组+链表+红黑树的方式实现的，初始化容量和每次扩容也都是需要开辟大量的前置空间来存储的。所以相比数组的方式可能还要更浪费内存。

### 总结

本文详细的介绍了Trie树相关的内容，Trie树作为一种特殊的数据结构，虽然在实际开发中并不常用，但其在特定的场景下比如suggest自动补全功能，可以发挥的出色的表现。Trie树本质上是一种通过空间换时间的策略，来提高检索性能，所以在使用的时候要注意内存限制。当然，Trie树在空间上也是有优化策略的，比如对部分前缀或者后缀进行压缩，这样以来能够节省不必要的指针存储，这种实现需要更复杂的编码来支持，感兴趣的朋友可以自己研究下。


















# Technique

```java

配置Nginx统一代理web容器如tomcat，jetty的请求，在日常开发中很常见，那么在配置集成的时候应该注意些什么呢?


下面我们将通过一个例子介绍如何和Nginx配置：

首先，我们先看下一个spring boot项目的结构：

```
demo
   src
       main
           assemble
                 package.xml
           filters
                 dev.properties
                 test.properties
                 pro.properties
           java
              dao
              service
              controller
           resources
              static
                    js
                    css
                    img
              templates
                    index.vm
                    error.vm
              application.properties
              logback.xml
              
       test       
   target
   logs
   pom.xml
   README.md
```

注意上面的是传统的spring boot的标准web格式，这里面包含了前端的页面也在这个项目里面，不是所谓的大前端的严格的前后端分离模式。如果集成了配置管理中心比如携程的阿波罗就可以再精简一点，这个不再细说，感兴趣的朋友可自行去研究一下。


在spring boot里面，默认的静态的资源文件是放在resources目录下一个static的目录下，如果现在在static目录有一个xxx.css文件，那么可以直接使用下面的方式进行访问：

```
http://localhost:8888/xxx.css
```

同理js，img，各种静态资源都一样。上面的方式是比较直接，但比较乱，因为在端口号后面的没有再分区路径。


现在看下我们的controller类：

```
@Controller
@RequestMapping("/c1")
public class DemoController {

    @RequestMapping("/m1")
    public String m1(Map<String, Object> model, HttpServletResponse response, HttpServletRequest request) {
        return "index";//返回到templates下面的index.vm模板
    }
    
    @RequestMapping("/m2")
    public String m2(Map<String, Object> model, HttpServletResponse response, HttpServletRequest request) {
        return "error";//返回到templates下面的error.vm模板
    }
    
}
```

在运维层面，一般只会为一个微服务项目配置一个域名映射，所以为了方便nginx统一代理映射，我们需要一个一级路径来做nginx的代理转发路径，故在上面的DemoController类里面，在类的注解上加上了一级路径，你也可以在方法级别添加比如直接写
```
/c1/m1
/c1/m2
```
效果是一样的。

下面我们看下nginx的配置：

```
  location ^~ /c1/ {
    access_log  /data/logs/nginx/c1.log  main;
    proxy_set_header   X-Real-IP        $remote_addr;
    proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
    proxy_set_header Host $host;
    proxy_pass http://c1;
  }
  
upstream question-recognition {
 server 192.168.10.125:8888 max_fails=3;
}
  
```

在上面的nginx配置中，我们配置了c1作为demo项目访问的一级路径，也就是说任何访问请求，都必须在c1的路径下，否则就可能出现问题，比如此时我访问：
```
http://192.168.10.125:8888/c1/m1
```
那么将会转发到index.vm页面，但此时注意，如果index.vm页面里面如下引用了xxx.js文件，那么将会失效：

```js
<script src="xxx.js" > </script>
```

上面的js文件，在html页面在浏览器加载的时候，会发送请求：
```
http://192.168.10.125:8888/xxx.js
```

注意了，如果没有配置nginx代理这一层，上面的这个请求是没问题的，但配置了nginx拦截，必须只能是c1路径下的请求才会放行，所以我们需要在static目录下同样新建一个c1目录，然后把xxx.js文件拷贝到该目录下即可，如下：

```
static
    c1
      xxx.js

```

然后在velocity的模板文件里面，引入js路径改为：

```
<script src="/c1/xxx.js" > </script>
```
上面的js文件，在html加载的时候，会发送请求：
```
http://192.168.10.125:8888/c1/xxx.js
```
由于其一级路径是c1，所以通过了nginx的拦截，整个页面就加载正常，这样以来与Nginx的集成就算成功了，这一点也是要在开发中注意的，通过了细化路径，也方便了访问权限的拦截控制。

```
根据群友反馈，还有另外一种方式：
在application.properties定义：
```
server.context-path= /app # Context path of the application.
````
应用的上下文路径，也可以称为项目路径，是构成url地址的一部分。


这样以来/app就构成了，整个spring boot项目的的访问的一级目录，所有的url访问，都必须在端口后面加上这个路径，同理静态资源目录也一样，需要建一个叫app的目录来保持统一，这样也方便使用nginx对spring-boot项目做映射


# Share

### 前言

十多年前，谷歌发布了大名鼎鼎的"三驾马车"的论文，分别是GFS(2003年)，MapReduce（2004年），BigTable（2006年），为开源界在大数据领域带来了无数的灵感，其中在 “BigTable” 的论文中很多很酷的方面之一就是它所使用的文件组织方式，这个方法更一般的名字叫 Log Structured-Merge Tree。在面对亿级别之上的海量数据的存储和检索的场景下，我们选择的数据库通常都是各种强力的NoSQL，比如Hbase，Cassandra，Leveldb，RocksDB等等，这其中前两者是Apache下面的顶级开源项目数据库，后两者分别是Google和Facebook开源的数据库存储引擎。而这些强大的NoSQL数据库都有一个共性，就是其底层使用的数据结构，都是仿照“BigTable”中的文件组织方式来实现的，也就是我们今天要介绍的LSM-Tree。

### 什么是LSM-Tree
LSM-Tree全称是Log Structured Merge Tree，是一种分层，有序，面向磁盘的数据结构，其核心思想是充分了利用了，磁盘批量的顺序写要远比随机写性能高出很多，如下图示：

[img01]

围绕这一原理进行设计和优化，以此让写性能达到最优，正如我们普通的Log的写入方式，这种结构的写入，全部都是以Append的模式追加，不存在删除和修改。当然有得就有舍，这种结构虽然大大提升了数据的写入能力，却是以牺牲部分读取性能为代价，故此这种结构通常适合于写多读少的场景。故LSM被设计来提供比传统的B+树或者ISAM更好的写操作吞吐量，通过消去随机的本地更新操作来达到这个目标。这里面最典型的例子就属于Kakfa了，把磁盘顺序写发挥到了极致，故而在大数据领域成为了互联网公司标配的分布式消息中间件组件。

虽然这种结构的写非常简单高效，但其缺点是对读取特别是随机读很不友好，这也是为什么日志通常用在下面的两种简单的场景：

（1） 数据是被整体访问的，大多数数据库的WAL（write ahead log）也称预写log，包括mysql的Binlog等

（2） 数据是通过文件的偏移量offset访问的，比如Kafka。

想要支持更复杂和高效的读取，比如按key查询和按range查询，就得需要做一步的设计，这也是LSM-Tree结构，除了利用磁盘顺序写之外，还划分了内存+磁盘多层的合并结构的原因，正是基于这种结构再加上不同的优化实现，才造就了在这之上的各种独具特点的NoSQL数据库，如Hbase，Cassandra，Leveldb，RocksDB，MongoDB，TiDB等。


### SSTable 和 LSM-Tree

提到LSM-Tree这种结构，就得提一下LevelDB这个存储引擎，我们知道Bigtable是谷歌开源的一篇论文，很难接触到它的源代码实现。如果说Bigtable是分布式闭源的一个高性能的KV系统，那么LevelDB就是这个KV系统开源的单机版实现，最为重要的是LevelDB是由Bigtable的原作者 Jeff Dean 和 Sanjay Ghemawat 共同完成，可以说高度复刻了Bigtable 论文中对于其实现的描述。


在LSM-Tree里面，核心的数据结构就是SSTable，全称是Sorted String Table，SSTable的概念其实也是来自于 Google 的 Bigtable 论文，论文中对 SSTable 的描述如下：

```
An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key/value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk. Optionally, an SSTable can be completely mapped into memory, which allows us to perform lookups and scans without touching disk.

```
如上所述，SSTable是一种拥有持久化，有序且不可变的的键值存储结构，它的key和value都是任意的字节数组，并且了提供了按指定key查找和指定范围的key区间迭代遍历的功能。SSTable内部包含了一系列可配置大小的Block块，典型的大小是64KB，关于这些Block块的index存储在SSTable的尾部，用于帮助快速查找特定的Block。当一个SSTable被打开的时候，index会被加载到内存，然后根据key在内存index里面进行一个二分查找，查到该key对应的磁盘的offset之后，然后去磁盘把响应的块数据读取出来。当然如果内存足够大的话，可以直接把SSTable直接通过MMap的技术映射到内存中，从而提供更快的查找。
[img02]

在LSM-Tree里，SSTable有一份在内存里面，其他的多级在磁盘上，如下图是一份完整的LSM-Tree图示：

[img03]

我们总结下在在LSM-Tree里面如何写数据的？

1，当收到一个写请求时，会先把该条数据记录在WAL Log里面，用作故障恢复。

2，当写完WAL Log后，会把该条数据写入内存的SSTable里面（删除是墓碑标记，更新是新记录一条的数据），也称Memtable。注意为了维持有序性在内存里面可以采用红黑树或者跳跃表相关的数据结构。

3，当Memtable超过一定的大小后，会在内存里面冻结，变成不可变的Memtable，同时为了不阻塞写操作需要新生成一个Memtable继续提供服务。

4，把内存里面不可变的Memtable给dump到到硬盘上的SSTable层中，此步骤也称为Minor Compaction，这里需要注意在L0层的SSTable是没有进行合并的，所以这里的key range在多个SSTable中可能会出现重叠，在层数大于0层之后的SSTable，不存在重叠key。

5，当每层的磁盘上的SSTable的体积超过一定的大小或者个数，也会周期的进行合并。此步骤也称为Major Compaction，这个阶段会真正 的清除掉被标记删除掉的数据以及多版本数据的合并，避免浪费空间，注意由于SSTable都是有序的，我们可以直接采用merge sort进行高效合并。


接着我们总结下在LSM-Tree里面如何读数据的？

1，当收到一个读请求的时候，会直接先在内存里面查询，如果查询到就返回。

2，如果没有查询到就会依次下沉，知道把所有的Level层查询一遍得到最终结果。



思考查询步骤，我们会发现如果SSTable的分层越多，那么最坏的情况下要把所有的分层扫描一遍，对于这种情况肯定是需要优化的，如何优化？在 Bigtable 论文中提出了几种方式：

1，压缩

SSTable 是可以启用压缩功能的，并且这种压缩不是将整个 SSTable 一起压缩，而是根据 locality 将数据分组，每个组分别压缩，这样的好处当读取数据的时候，我们不需要解压缩整个文件而是解压缩部分 Group 就可以读取。

2，缓存

因为SSTable在写入磁盘后，除了Compaction之外，是不会变化的，所以我可以将Scan的Block进行缓存，从而提高检索的效率

3，索引，Bloom filters

正常情况下，一个读操作是需要读取所有的 SSTable 将结果合并后返回的，但是对于某些 key 而言，有些 SSTable 是根本不包含对应数据的，因此，我们可以对每一个 SSTable 添加 Bloom Filter，因为布隆过滤器在判断一个SSTable不存在某个key的时候，那么就一定不会存在，利用这个特性可以减少不必要的磁盘扫描。


4，合并

这个在前面的写入流程中已经介绍过，通过定期合并瘦身， 可以有效的清除无效数据，缩短读取路径，提高磁盘利用空间。但Compaction操作是非常消耗CPU和磁盘IO的，尤其是在业务高峰期，如果发生了Major Compaction，则会降低整个系统的吞吐量，这也是一些NoSQL数据库，比如Hbase里面常常会禁用Major Compaction，并在凌晨业务低峰期进行合并的原因。



最后有的同学可能会问道，为什么LSM不直接顺序写入磁盘，而是需要在内存中缓冲一下？ 这个问题其实很容易解答，单条写的性能肯定没有批量写来的块，这个原理其实在Kafka里面也是一样的，虽然kafka给我们的感觉是写入后就落地，但其实并不是，本身是 可以根据条数或者时间比如200ms刷入磁盘一次，这样能大大提升写入效率。此外在LSM中，在磁盘缓冲的另一个好处是，针对新增的数据，可以直接查询返回，能够避免一定的IO操作。



### B+Tree VS LSM-Tree

传统关系型数据采用的底层数据结构是B+树，那么同样是面向磁盘存储的数据结构LSM-Tree相比B+树有什么异同之处呢？

LSM-Tree的设计思路是，将数据拆分为几百M大小的Segments，并是顺序写入。

B+Tree则是将数据拆分为固定大小的Block或Page, 一般是4KB大小，和磁盘一个扇区的大小对应，Page是读写的最小单位。

在数据的更新和删除方面，B+Tree可以做到原地更新和删除，这种方式对数据库事务支持更加友好，因为一个key只会出现一个Page页里面，但由于LSM-Tree只能追加写，并且在L0层key的rang会重叠，所以对事务支持较弱，只能在Segment Compaction的时候进行真正地更新和删除。

因此LSM-Tree的优点是支持高吞吐的写（可认为是O（1）），这个特点在分布式系统上更为看重，当然针对读取普通的LSM-Tree结构，读取是O（N）的复杂度，在使用索引或者缓存优化后的也可以达到O（logN）的复杂度。

而B+tree的优点是支持高效的读（稳定的OlogN），但是在大规模的写请求下（复杂度O(LogN)），效率会变得比较低，因为随着insert的操作，为了维护B+树结构，节点会不断的分裂和合并。操作磁盘的随机读写概率会变大，故导致性能降低。


还有一点需要提到的是基于LSM-Tree分层存储能够做到写的高吞吐，带来的副作用是整个系统必须频繁的进行compaction，写入量越大，Compaction的过程越频繁。而compaction是一个compare & merge的过程，非常消耗CPU和存储IO，在高吞吐的写入情形下，大量的compaction操作占用大量系统资源，必然带来整个系统性能断崖式下跌，对应用系统产生巨大影响，当然我们可以禁用自动Major Compaction，在每天系统低峰期定期触发合并，来避免这个问题。

阿里为了优化这个问题，在X-DB引入了异构硬件设备FPGA来代替CPU完成compaction操作，使系统整体性能维持在高水位并避免抖动，是存储引擎得以服务业务苛刻要求的关键。


### 总结

本文主要介绍了LSM-Tree的相关内容，简单的说，其牺牲了部分读取的性能，通过批量顺序写来换取了高吞吐的写性能，这种特性在大数据领域得到充分了体现，最直接的例子就各种NoSQL在大数据领域的应用，学习和了解LSM-Tree的结构将有助于我们更加深入的去理解相关NoSQL数据库的实现原理，掌握隐藏在这些框架下面的核心知识。
















