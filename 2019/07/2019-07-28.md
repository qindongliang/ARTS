
 [Algorithm CanPlaceFlowers ](#algorithm)

 [Review 在Java里面如何解决进退两难的jar包冲突问题？](#review)

 [Technique Java基本类型的内存分配在栈还是堆？](#technique)

 [Share 如何动手撸一个简单的LFU缓存](#share)


# Algorithm

```java
package leetcode.easy.array_all;

/****
 * https://leetcode.com/problems/can-place-flowers/
 *
 * 给定一个int数组，代表花罐子，里面的数值只能是0或者1，0代表空，1代表有花
 * 只有两个0之间才能种花，如果两个1挨着，会导致他们争夺水资源导致死亡。
 * 现在给定一个花罐数组，并给出新种的花的个数，让求出，这个罐子中能不能放的下新增的花。
 *
 * 思路：
 *
 * 循环数组，初始化情况下count=1，然后对连续0计数，如果遇到非0，就count-1的结果除以2，就是能中的颗数，
 * 直到循环结束，最后在除以2，看商是否大于等于n，如果满足，则可以中下。
 */
public class CanPlaceFlowers {
    public static boolean canPlaceFlowers(int[] flowerbed, int n) {

        int count = 1;
        int result = 0;
        for(int i=0; i<flowerbed.length; i++) {
            if(flowerbed[i] == 0) {
                count++;
            }else {
                result += (count-1)/2;
                count = 0;
            }
        }
        if(count != 0) result += count/2;
        return result>=n;
    }
    public static void main(String[] args) {
        int arr[]={0,0};
        System.out.println(canPlaceFlowers(arr,1));
    }
}

```

# Review


es api组件依赖guava18.0，spark项目由于业务需要写入es所以需要依赖es ，但spark项目的环境又需要依赖guava14.0，如果换成高版本可能会报错，这个决定了你不能都使用统一的低版本或者高版本来规避此问题，因此必须面对现实。


导致异常的原因简单说下：

spark环境首先启动，导致jvm里面已经加载了guava14.0，这个时候jvm不会加载es依赖的guava18.0，而当es初始化的时候，恰巧需要使用guava18.0新版本的api，而这个api在14.0里面却并不存在，这个时候就会发生异常，就是我们常看到的：

```
java.lang.NoSuchMethodException
```
在深入了解一下，为什么会发生这个异常？是因为java里面的类加载器是双亲委派模式，一个类只需要在双亲委派模式下正常加载过（唯一全限定名：包名+类名）一次，就不会重复加载，从而引发了上面的问题。想要解决这种问题，靠重新再写一个类加载器是不现实的，因为重新写一个类加载器，不遵守双亲委派模式，就相当于把环境隔离了，技术上可行，但没法解决问题，如果A加载器加载的类，要调B加载器里面的类，或者B调A，会引发新的依赖问题。

那么如何比较优雅的解决这种进退两难的困境问题呢？ maven-shade-plugin的出现，就可以解决这个问题的。它的解决手段也非常简单，前面说明JVM类加载器只会加载某个类一次，是通过全路径的包名+类名来区分做到的，我们要想加载不同版本的同一个类，有两种简单的方式，第一种改类名，第二种改包名。综合考虑来说改包名是最为妥当的一种方式，如果改了类名，那么要修改和替换的地方就要比改包名复杂的多了，不仅类调用的每一个地方都要替换，另外包名导入的地方也需要替换（.*导入除外，现实中不建议用这种方式），而修改包名，只需要把每一个依赖该类的类文件头部导入路径调换成新的即可，文件里面的类无需修改。

通过maven-shade-plugin插件的功能，就可以很容易做到这件事。

解法是：

单独为es的依赖创建一个maven项目，然后pom里面引入依赖的es组件，并对es组件里面依赖的guava的包名和部分组件，进行shade修改，如下：


```
    <groupId>es.shade</groupId>
    <artifactId>es-shade</artifactId>
    <version>1.0-SNAPSHOT</version>
    <properties>
        <elasticsearch.version>2.3.4</elasticsearch.version>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.elasticsearch</groupId>
            <artifactId>elasticsearch</artifactId>
            <version>${elasticsearch.version}</version>
        </dependency>

    </dependencies>
    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.1</version>
                <configuration>
                    <createDependencyReducedPom>false</createDependencyReducedPom>
                </configuration>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <relocations>
                                <relocation>
                                    <pattern>com.google.guava</pattern>
                                    <shadedPattern>my.elasticsearch.guava</shadedPattern>
                                </relocation>
                                <relocation>
                                    <pattern>org.joda</pattern>
                                    <shadedPattern>my.elasticsearch.joda</shadedPattern>
                                </relocation>
                                <relocation>
                                    <pattern>com.google.common</pattern>
                                    <shadedPattern>my.elasticsearch.common</shadedPattern>
                                </relocation>
                                <relocation>
                                    <pattern>com.google.thirdparty</pattern>
                                    <shadedPattern>my.elasticsearch.thirdparty</shadedPattern>
                                </relocation>
                            </relocations>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer" />
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
```
通过这样修改后再进行打包，那么相当于把所有guava这个改名后的组件和es的依赖在编译后的class文件层进行绑定，将其两者变成一个整体依赖jar，并且这个组件也会自动修改es里面所有导入guava的旧路径为改动后的新路径，看如下从反编译后的jar中，拷贝出来的类文件信息：

改动后的guava的MoreExecutors这个类文件头部，变成了我们修改后的包名：


```java
package my.elasticsearch.common.util.concurrent;

import my.elasticsearch.common.collect.*;
import java.lang.reflect.*;
import my.elasticsearch.common.base.*;
import my.elasticsearch.common.annotations.*;
import java.util.concurrent.locks.*;
import java.util.*;
import java.util.concurrent.*;

public final class MoreExecutors
{
// 省略主体内容
}
```

注意头部的包名已经变成了my.elasticsearch.common替代了原来的com.google.common。

下面我们再看下es源码里面依赖的guava路径是否变化，打开org.elasticsearch.threadpool.ThreadPool这个类，我们发现其头部已经变成了新的guava路径，如下：

```java
package org.elasticsearch.threadpool;

import org.elasticsearch.common.component.*;
import org.elasticsearch.node.settings.*;
import org.apache.lucene.util.*;
import my.elasticsearch.common.collect.*;
import org.elasticsearch.common.*;
import org.elasticsearch.common.util.concurrent.*;
import org.elasticsearch.common.unit.*;
import org.elasticsearch.common.collect.*;
import org.elasticsearch.common.settings.*;
import java.util.concurrent.*;
import org.elasticsearch.common.logging.*;
import my.elasticsearch.common.util.concurrent.*;
import java.io.*;
import org.elasticsearch.common.io.stream.*;
import org.elasticsearch.common.xcontent.*;
import org.elasticsearch.cluster.settings.*;
import org.elasticsearch.cluster.*;
import java.util.*;
import java.util.regex.*;

public class ThreadPool extends AbstractComponent
{

// 省略主体内容
}
```

如此已来，这个shade jar里面的es就只对这个版本的guava进行了绑定依赖，这个时候在spark项目中，引入这个es的uber-shade-jar，就不会发生冲突，通过使用不同的包名完美解决了类冲突的问题，这两个类都可以被同一个JVM虚拟机加载，这样以来，spark仍旧可以使用guava14.0版本，而我们的es也可以完美的使用改名后的guava18.0的版本，从而比较优雅的解决了这种不可避免的多版本冲突问题。

https://stackoverflow.com/questions/13620281/what-is-the-maven-shade-plugin-used-for-and-why-would-you-want-to-relocate-java



























# Technique

![image](https://1.bp.blogspot.com/-gKWUcwIKWWU/VvPtKUAIFjI/AAAAAAAAFRc/WLCqWfSxlZ4ioocmBuFS3KaRhzs0I13OA/s640/Difference%2Bbetween%2Bstack%2Band%2Bheap%2Bmemory%2Bin%2BJava.gif)

我们都知道在Java里面new出来的对象都是在堆上分配空间存储的，但是针对基本类型却有所区别，基本类型可以分配在栈上，也可以分配在堆上，这是为什么？

在这之前，我们先看下Java的基本类型8种分别是：

```
byte =>8bit
short => 16bit
int => 32bit
long =>64bit

folat => 单精度32位
double => 双精度64位

boolean => 注意oracle官网文档介绍，boolean代表1bit的信息，但它本身的size却不是精确的，依赖于jvm和os的实现，比较常见的说法是，boolean单独使用的时候，在编译时是使用int代替的，如果是boobean数组，则是使用1byte代替

char => 16bit

```
注意字符串内部是用char数组实现的，所以属于引用类型。


基本类型在成员变量和局部（local）变量的时候其内存分配机制是不一样的。

如果是成员变量，那么不分基本类型和引用类型都是在java的堆内存里面分配空间，而局部变量的基本类型是在栈上分配的。栈属于线程私有的空间，局部变量的生命周期和作用域一般都很短，为了提高gc效率，所以没必要放在堆里面。

如下代码：

```
public class DemoTest {

    int y;// 分布在堆上
    public static void main(String[] args) {

        int x=1; //分配在栈上
        String name=new String("cat");//数据在堆上，name变量的指针在栈上
        String address="北京";//数据在常量池，属于堆空间，指针在栈
        Integer price=4;//包装类型同样是引用类型，编译时会自动装拆相，所以数据在堆上，指针在栈
    }


}
```

在java里面通过new出来的对象都在堆上分配，这里有两种特殊情况，

（1）字符串的字面量

字符串的字面量，没有new关键字，但却是在堆上分配内存的，严格的说是在堆里面的字符串常量池里面。

（2）基本类型的包装类

同样的道理，针对各个基本类型的包装类型，如：Integer，Double，Long等，这些属于引用类型，我们直接在局部方法里面使用包装类型赋值，那么数据真正的内存分配还是在堆内存里面，这里有个隐式的拆装箱来自动完成转换，数据的指针是在栈上，包装类型的出现主要是为了基本类型能够用在泛型的设计上和使用null值，而基本类型则拥有更好的计算性能，这一点我们也需要注意。






# Share

前面的文章已经介绍过，关于缓存置换算法的来由和理论知识，如果还没了解的同学，可点击下面的链接进行查看：

https://mp.weixin.qq.com/s?__biz=MzAxMzE4MDI0NQ==&mid=2650336691&idx=1&sn=d31b3ba325affdc51d0391a595a89b58&chksm=83aac189b4dd489f5ae9854f488163613facd06714540317018b5a58a452cd04bc41c73a4538&scene=21#wechat_redirect

https://mp.weixin.qq.com/s?__biz=MzAxMzE4MDI0NQ==&mid=2650336679&idx=1&sn=1b727d7ddc6c069627d7b8fbb444aaf4&chksm=83aac19db4dd488b5186ed9d7f1ff14b89234ac8479708b89b107de39f173adef227921cdcf3&scene=21#wechat_redirect

今天我们来看下，如何用代码来实现一个简单的LFU缓存。

我们知道缓存置换算法主流的有三种，分别是：

(1) FIFO：First In First Out，先进先出策略

(2) LFU：Least Frequently Used，最不经常使用策略

(3) LRU：Least Recently Used，最近最少使用策略

关于第一种FIFO策略的实现，比较简单，可采用固定长度的数组和链表来处理，这里就不重点说了。 今天我们的重点是LFU缓存的实现。

LFU 全称 Least Frequently Used，从名字上我们就能看出来这个算法是基于数据访问频率（次数）来淘汰数据的，也就是说系统会记录一段时间内所有数据的访问次数，当缓存区满的时候，会优先淘汰访问次数最少的数据。 其核心思想：如果一个数据在最近一段时间内访问次数很少，则在将来一段时间内被访问的可能性也很小。显然，这是一种合理的算法，因为到目前为止最少使用的页面，很可能也是将来最少访问的页面。缓存的每个数据都有引用计数，所有数据按照引用计数排序，具有相同引用计数的数据按照时间排序。


我们来看下LFU的实现代码：


```java
package algorithm.cache_algorithm;

import java.util.Collections;
import java.util.HashMap;
import java.util.Map;

/****
 * 实现一个 LFU 缓存
 * Least Frequently Used，最不经常使用策略
 */
public class LFUCache {

    private   int capacity;//缓存队列的容量值

    private Map<Integer,AccessRate> cache;//保存缓存数据

    public LFUCache(int capacity) {
        this.capacity = capacity;
        cache=new HashMap<>();
    }

    public void put(int key,int value){
        AccessRate v=cache.get(key);
        if(v==null){//第一次插入

            if(cache.size()==capacity){//容量已超
                cache.remove(getEvictKey());//驱逐被淘汰的数据
            }
            //新增
            v=new AccessRate(key,value,1,System.nanoTime());
            cache.put(key,v);
            log("add",v);
        }else{
            //更新状态
            v.value=value;
            v.hitCount=v.hitCount+1;
            v.lastTime=System.nanoTime();
            cache.put(key,v);
            log("update",v);
        }


    }



    public int get(int key){
        AccessRate v=cache.get(key);
        if(v!=null){
            v.hitCount=v.hitCount+1;
            v.lastTime=System.nanoTime();
            log("query",v);
            return v.value;
        }
        log("query",new AccessRate(key));
        return -1;
    }

    public void remove(int key){
        AccessRate v=cache.remove(key);
        if(v!=null){
            log("remove",v);
        }else{
            log("remove",new AccessRate(key));
        }

    }


    //记录操作明细，方便理解
    public void log(String operator,AccessRate accessRate){
        if(accessRate.isNotNull()){
            System.out.println(operator+" => "+accessRate.toString()+" "+deatail());
        }else {
            System.out.println(operator+" => the key "+accessRate.key+" not exist"+" "+deatail());
        }


    }

    public String deatail(){
        StringBuilder sb=new StringBuilder();
        sb.append("当前缓存中有效的key=");
        for (AccessRate accessRate:cache.values()){
            sb.append(accessRate.key+"，");
        }
        return sb.deleteCharAt(sb.length()-1).toString();
    }

    //获取缓存里面，需要被淘汰的数据，复杂度O（N），可用优先级队列（堆）进行优化
    public Integer getEvictKey(){
        AccessRate min= Collections.min(cache.values());
        log("evict",min);
        return min.key;
    }




    class AccessRate implements Comparable<AccessRate> {

        private   Integer key;//访问的数据key
        private   Integer value;//访问数据的value
        private   Integer hitCount;//记录访问次数
        private   Long lastTime;//最新的访问时间



        public AccessRate(Integer key, Integer value, Integer hitCount, Long lastTime) {
            this.key = key;
            this.value = value;
            this.hitCount = hitCount;
            this.lastTime = lastTime;
        }

        public AccessRate() {
        }

        public AccessRate(Integer key) {
            this.key=key;
        }

        public boolean isNotNull(){
            return key!=null&&value!=null;
        }

        @Override
        public int compareTo(AccessRate o) {
            int firstSort=hitCount.compareTo(o.hitCount);
            if(firstSort!=0){////如果命中次数不相等，使用命中次数排序
                return firstSort;
            }else {//如果命中次数相等，就使用访问的时间进行排序
                return lastTime.compareTo(o.lastTime);
            }
        }

        @Override
        public String toString() {
            return "AccessRate{" +
                    "key=" + key +
                    ", value=" + value +
                    ", hitCount=" + hitCount +
                    ", lastTime=" + lastTime +
                    '}';
        }
    }

}

```

代码并不复杂，为了方便大家观察到细节，我特意在代码里面加了相关log输出，下面我们来测试这个缓存算法：


```java
    public static void main(String[] args) {
        //构建一个容量为3的LFU缓存实例
        LFUCache cache=new LFUCache(3);

        cache.put(1,1);// 添加 1
        cache.put(2,2);// 添加 2
        cache.get(1);// 访问 1
        cache.get(2);// 访问 2
        cache.put(3,3);// 添加 3
        cache.put(4,4);// 缓存满，先淘汰访问次数少的3，然后再添加 4
        cache.get(5);// 访问 5
        cache.remove(4); // 移除 4
        cache.remove(4);// 移除 4

    }
```

完整代码，请访问我的github：
https://github.com/qindongliang/Java-Note/blob/master/src/main/java/algorithm/cache_algorithm/LFUCache.java 

运行上面的代码，我们的控制台输出信息如下：

```
add => AccessRate{key=1, value=1, hitCount=1, lastTime=113379228623808} 当前缓存中有效的key=1
add => AccessRate{key=2, value=2, hitCount=1, lastTime=113379229329163} 当前缓存中有效的key=1，2
query => AccessRate{key=1, value=1, hitCount=2, lastTime=113379229389607} 当前缓存中有效的key=1，2
query => AccessRate{key=2, value=2, hitCount=2, lastTime=113379229445363} 当前缓存中有效的key=1，2
add => AccessRate{key=3, value=3, hitCount=1, lastTime=113379229493594} 当前缓存中有效的key=1，2，3
evict => AccessRate{key=3, value=3, hitCount=1, lastTime=113379229493594} 当前缓存中有效的key=1，2，3
add => AccessRate{key=4, value=4, hitCount=1, lastTime=113379229660888} 当前缓存中有效的key=1，2，4
query => the key 5 not exist 当前缓存中有效的key=1，2，4
remove => AccessRate{key=4, value=4, hitCount=1, lastTime=113379229660888} 当前缓存中有效的key=1，2
remove => the key 4 not exist 当前缓存中有效的key=1，2
```

可以看到结果是没问题的，在上面的测试中，我们设置缓存的容量大小为3，然后先添加了两条数据1，2，然后接着分别对1和2进行了查询，注意这个时候1和2的引用计数会增加2，并且他们的时间也会更新，接着我们添加了3和4，注意在添加4的时候由于缓存容量已经满了，为了能让4添加进来，我们必须根据淘汰一条数据，那么这个淘汰数据应该是谁呢？ 毫无疑问就是3了，因为3的访问次数最少。注意如果这里3的次数也是2，那么算法会根据时间选择一条时间最早的数据，这个时候淘汰的数据就是1了，最后我们访问了一条不存在的数据5，并且对同一个key=4的数据，删除了2次，可以看到结果也是没有问题的。



该算法的时间复杂度最坏的情况为O（N），原因在于在淘汰数据的时候，我们偷了个懒，用的是Collections.min方法，这个函数内部会迭代整个values里面的数据来找到需要被淘汰的数据，当数据量大的时候性能不会太好，所以在这个地方，我们可以使用堆这种数据结构来优化性能，从而让时间复杂度降至O（logN），如果是在Java里面，我们可以直接借助优先级队列（底层结构堆）来实现，并提供相关的自定义排序策略。


本文主要介绍了LFU缓存算法的简单实现和复杂度分析，LFU算法可以避免偶发性的、周期性的批量操作会导致LRU算法命中率急剧下降，缓存污染情况比较严重的问题。但其缺点也很明显，面对一段时间内热点数据，其效果没有LRU好，LFU在存在大量的历史数据的高频访问时，如果此时新来了很多访问频次略低于历史数据的时候，新的热点数据由于频次略低，在容量有限的时候很有可能就被淘汰了，从而造成缓存miss，此外从实现的复杂度上来分析，LFU 需要维护一个队列记录所有数据的访问记录，每个数据都要维护引用计数，内存消耗和性能消耗都较高。LFU整体上在空间和时间复杂度上均高于LRU算法，这也是为什么LRU算法更受欢迎的原因，在下篇文章我们会重点介绍下如何实现一个LRU缓存。








